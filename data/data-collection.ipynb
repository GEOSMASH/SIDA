{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.11 64-bit ('gds': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0a14a7283c5c5864b815aee9a8006f6c118a5518b0596da4aeb02728445e16d0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy.sparse as sp\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm, trange\n",
    "from cenpy.products import ACS\n",
    "from download_lodes import download_lodes\n",
    "\n",
    "acs = ACS()\n",
    "\n",
    "states = ['al', 'az', 'ar', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'id', 'il', 'in', 'ia', \n",
    "          'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn', 'ms', 'mo', 'mt', 'ne', 'nv', 'nh', \n",
    "          'nj', 'nm', 'ny', 'nc', 'nd', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', \n",
    "          'ut', 'vt', 'va', 'wa', 'wv', 'wi', 'wy']\n",
    "statestrs = ['Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida', 'Georgia', \n",
    "             'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', \n",
    "             'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', \n",
    "             'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', \n",
    "             'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "# create lambda function for aggregation \n",
    "bg_to_cty = lambda geoid : geoid[:5]  # converts census block group FIPS code to county FIPS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all geoids from county_adjacency.txt\n",
    "cties = pd.read_csv('county_adjacency.txt', sep=r'\\t', header=None)\n",
    "geoids = pd.unique(cties[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrices -- DO NOT RUN THIS CELL AGAIN\n",
    "OD = pd.DataFrame(columns=geoids, index=geoids).fillna(0)\n",
    "OD.to_csv('total_od.csv')\n",
    "\n",
    "attrs = pd.DataFrame(columns=['o_attr', 'd_attr'], index=geoids).fillna(0)\n",
    "attrs.to_csv('total_attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data matrices\n",
    "OD = pd.read_csv('total_od.csv')\n",
    "attrs = pd.read_csv('total_attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 49/49 [14:45:36<00:00, 1084.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get all intra-state flows TODO speed up any of these loops??\n",
    "for state in tqdm(states):\n",
    "    filenames = download_lodes(state, contained=True)  # download LODES data within a state\n",
    "\n",
    "    # Aggregate census block group data to counties\n",
    "    converters = {'w_geocode' : str, 'h_geocode' : str}\n",
    "    odfile = dd.read_csv(filenames['od'], compression='gzip', converters=converters)\n",
    "    for _, row in odfile.iterrows():\n",
    "        OD.loc[bg_to_cty(row['w_geocode']), bg_to_cty(row['h_geocode'])] += row['S000']\n",
    "\n",
    "    # Scrape origin attributes\n",
    "    oattrfile = dd.read_csv(filenames['rac'], compression='gzip', converters={'h_geocode' : str})\n",
    "    for _, row in oattrfile.iterrows(): attrs.loc[bg_to_cty(row['h_geocode'])] += row['C000']\n",
    "    \n",
    "    # Scrape destination attributes\n",
    "    dattrfile = dd.read_csv(filenames['wac'], compression='gzip', converters={'w_geocode' : str})\n",
    "    for _, row in dattrfile.iterrows(): attrs.loc[bg_to_cty(row['w_geocode'])] += row['C000']\n",
    "\n",
    "    for v in filenames.values(): os.remove(v)  # delete old archives\n",
    "\n",
    "    # Save your work!\n",
    "    OD.to_csv('total_od.csv')\n",
    "    attrs.to_csv('total_attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 49/49 [49:33<00:00, 60.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get all extra-state flows (attributes have already been recorded)\n",
    "for state in tqdm(states):\n",
    "    filenames = download_lodes(state, contained=False)  # download extra-state LODES data\n",
    "\n",
    "    # Aggregate census block group data to counties\n",
    "    converters = {'w_geocode' : str, 'h_geocode' : str}\n",
    "    odfile = dd.read_csv(filenames['od'], compression='gzip', converters=converters)\n",
    "    for _, row in odfile.iterrows():\n",
    "        OD.loc[bg_to_cty(row['w_geocode']), bg_to_cty(row['h_geocode'])] += row['S000']\n",
    "\n",
    "    os.remove(filenames['od'])  # delete old archives\n",
    "\n",
    "    # Save your work!\n",
    "    OD.to_csv('total_od.csv')"
   ]
  },
  {
   "source": [
    "## Reformat data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data matrices\n",
    "OD = pd.read_csv('total_od.csv')       # i forgot to read these in with the index column but one should do that normally!!\n",
    "geoids = np.delete(OD.columns.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cty_shapes = gpd.read_file('tl_2018_us_county.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new flow dataframe\n",
    "od = sp.coo_matrix(OD.values[:, 1:].astype(int))\n",
    "newOD = pd.DataFrame(np.array([od.row, od.col, od.data]).T, columns=['origin', 'dest', 'count'])\n",
    "mapping = {i : v for i, v in enumerate(geoids)}\n",
    "newOD = newOD.replace({'origin' : mapping, 'dest' : mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOD.to_csv('lodes-flows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id   name  o_attr  d_attr         lat        lon\n",
       "0        0  01001   35115   35115  -86.642749  32.534920\n",
       "1        1  01021   27140   27140  -86.718814  32.847853\n",
       "2        2  01047   26388   26388  -87.106476  32.325974\n",
       "3        3  01051   53498   53498  -86.149147  32.596648\n",
       "4        4  01085    5335    5335  -86.650108  32.154750\n",
       "...    ...    ...     ...     ...         ...        ...\n",
       "3109  3109  56043    7216    7216 -107.682861  43.904997\n",
       "3110  3110  56013   30700   30700 -108.630418  43.040528\n",
       "3111  3111  56025   79809   79809 -106.798494  42.962240\n",
       "3112  3112  56017    4207    4207 -108.442097  43.718929\n",
       "3113  3113  56035    7088    7088 -109.914706  42.766907\n",
       "\n",
       "[3114 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>o_attr</th>\n      <th>d_attr</th>\n      <th>lat</th>\n      <th>lon</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>01001</td>\n      <td>35115</td>\n      <td>35115</td>\n      <td>-86.642749</td>\n      <td>32.534920</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>01021</td>\n      <td>27140</td>\n      <td>27140</td>\n      <td>-86.718814</td>\n      <td>32.847853</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>01047</td>\n      <td>26388</td>\n      <td>26388</td>\n      <td>-87.106476</td>\n      <td>32.325974</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>01051</td>\n      <td>53498</td>\n      <td>53498</td>\n      <td>-86.149147</td>\n      <td>32.596648</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>01085</td>\n      <td>5335</td>\n      <td>5335</td>\n      <td>-86.650108</td>\n      <td>32.154750</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3109</th>\n      <td>3109</td>\n      <td>56043</td>\n      <td>7216</td>\n      <td>7216</td>\n      <td>-107.682861</td>\n      <td>43.904997</td>\n    </tr>\n    <tr>\n      <th>3110</th>\n      <td>3110</td>\n      <td>56013</td>\n      <td>30700</td>\n      <td>30700</td>\n      <td>-108.630418</td>\n      <td>43.040528</td>\n    </tr>\n    <tr>\n      <th>3111</th>\n      <td>3111</td>\n      <td>56025</td>\n      <td>79809</td>\n      <td>79809</td>\n      <td>-106.798494</td>\n      <td>42.962240</td>\n    </tr>\n    <tr>\n      <th>3112</th>\n      <td>3112</td>\n      <td>56017</td>\n      <td>4207</td>\n      <td>4207</td>\n      <td>-108.442097</td>\n      <td>43.718929</td>\n    </tr>\n    <tr>\n      <th>3113</th>\n      <td>3113</td>\n      <td>56035</td>\n      <td>7088</td>\n      <td>7088</td>\n      <td>-109.914706</td>\n      <td>42.766907</td>\n    </tr>\n  </tbody>\n</table>\n<p>3114 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "newloc = pd.read_csv('attrs.csv', converters={'name' : str})\n",
    "\n",
    "# Drop all non CONUS locs\n",
    "name_check = lambda series : pd.Series([int(n[:2]) for n in series])\n",
    "loc = newloc.drop((newloc[name_check(newloc['name']) > 56]).index)\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 49/49 [13:36<00:00, 16.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get pop data and add to attributes\n",
    "pops = {}\n",
    "for state in tqdm(statestrs):\n",
    "    df = acs.from_state(state, variables=['B00001_001E'], level='county')\n",
    "    tempdict = {id : df[df['GEOID'] == id]['B00001_001E'].values[0] for id in df['GEOID'].values}  # get all (geoid, value pairs)\n",
    "    pops.update(tempdict)  # add all (geoid, value) pairs to pops\n",
    "\n",
    "loc['pop'] = loc['name'].map(pops)  # add this as a column to the dataframe\n",
    "loc.to_csv('attrs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3114/3114 [01:44<00:00, 29.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add lat/lon to attributes\n",
    "lat = np.zeros((loc.shape[0], 1))\n",
    "lon = np.zeros((loc.shape[0], 1))\n",
    "for i in trange(loc.shape[0]):\n",
    "    name = loc['name'].iloc[i]\n",
    "    if name == '46113' or name == '51515': continue  # not sure why these are in here but skip them\n",
    "    lat[i] = cty_shapes[cty_shapes['GEOID'] == name].centroid.values.x\n",
    "    lon[i] = cty_shapes[cty_shapes['GEOID'] == name].centroid.values.y\n",
    "\n",
    "loc['lat'] = lat\n",
    "loc['lon'] = lon\n",
    "loc.to_csv('attrs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}